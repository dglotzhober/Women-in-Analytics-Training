{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Practice Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Clustering Practice Problem\n",
    "\n",
    "In this problem, we will be using data on mall shoppers to derive insights about consumer behavior at this mall.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages: It is best practice to import all of your packages at the top of a code file\n",
    "#Periodically return to this cell to import additional packages (make sure to re-run the code too)\n",
    "#Hint: For this exercise, you will need pandas, os, matplotlib, a 3D plotting package (we use mpl_toolkits.mplot 3d), and sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Visualize Data\n",
    "Import the \"Mall_Customers.csv\" file and create a 3D plot of the data using age and income as predictor variables and spending score as the response variable. Does it look like there is any customer segmentation? What are some initial insights you are able to derive from this plot alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working directory to where your csv file is stored on your laptop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset as a pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Perform initial visualization of data to assess potential clusters\n",
    "#Use Spending Score as the response variabe (z-axis) and Age/Annual Income (k$) on the x/y axes\n",
    "#An option for packages include: matplotlib pyplot and Axes3D from mpl_toolkits.mplot3d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does it look like there is any customer segmentation? What are some initial insights you are able to derive from this plot alone?\n",
    "#No code here, just think about it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: K-Means Clustering\n",
    "Use K-Means Clustering with k = 4 clusters on this dataset. Use the same predictor and response variables.\n",
    "\n",
    "Re-create the 3D plot from Part 1, but put each cluster group in its own color. If you were the mall owner, what insights would you derive from this data? How could this affect your business decisions/strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the subset of data to be used in clustering\n",
    "#Make a new pandas dataframe using the Age, Annual Income, and Spending Score fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a k-Means cluster model using the cluster.KMeans function from Sklearn\n",
    "#Collect the labels using the \".labels_\" attribute\n",
    "#Apply the labels to the dataset by creating a new dataframe and appending the labels as their own column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-create the 3D visualization with the clusters color-coded\n",
    "\n",
    "#Use lambda functions to create a pandas dataframe for each cluster, based on the value in the cluster column\n",
    "#Then, use Axes3D to simultaneously visualize the clusters and assign colors to the data points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Find Optimal Number of Clusters (K-Means)\n",
    "Using the Silhouette score, determine the optimal number of clusters for k = 2 to k = 8. Re-run the k-means clustering model on using this optimal number and create a visualization of the result. What improved insights can be derived from the updated, optimal graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list containing the number of clusters from 2 to 8\n",
    "#Loop through this list and run a k-means model on each\n",
    "#Calculate the Silhouette score for each model using the sklearn.metrics.silhouette_score function\n",
    "#Print each Silhouette value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-run clustering procedure with k=6 clusters, and create a dataframe of the data with cluster labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Clusters (you can use the same code as in part 2 but adjust the number of cluster dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What additional insights come from using this optimal value of k?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: DBSCAN Clustering\n",
    "Use DBSCAN clustering on these same three fields. Re-produce the 3D plot with the new clusters and calculate the Silhouette score for this model. Based on the visualization and the score, which clustering technique do you think is best for this particular problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DBSCAN model for this dataset\n",
    "#Tweaking the epsilon and min_samples values will be important here\n",
    "#Create another dataframe of the data + cluster labels\n",
    "#Look at the distribution of clusters to assess your choices of paramaters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What additional insights can you derive?\n",
    "#How does DBSCAN compare to k-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a Breast Cancer dataset to build a model that classifies tumor as cancerous or benign. We will use PCA on this data to reduce dimensions and potentially help us improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Breast Cancer dataset from Sklearn. This dataset has the type of tumor (1=cancerous, 0=benign), as well as many attributes of the tumor. Note that when you import the dataset, it will import as a dictionary of dataframes (using as_frame = True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import sklearn breast cancer data set:\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "#Import other data sets\n",
    "#We will be using pandas, sklearn, matplotlib, numpy, and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the breast cancer data from a dictionary into a dataframe\n",
    "data_dict=load_breast_cancer(as_frame=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform exploratory analysis on the target variable: What are the datatypes for the target series and the dataframe? How many of the data points are cancerous? Etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find data types of data set and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the number of cancerous data points, and what percent of all data points are cancerous\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classification Model\n",
    "Build a classification model of your choice using Sklearn.\n",
    "Make sure that you first normalize the dataset.\n",
    "Make sure that you record: 1) the runtime of the model, and 2) the model accuracy.\n",
    "This solution will be using a Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize dataset using sklearn.preprocessing.StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a basic logistic regression model (or other classification model)\n",
    "#Print the training time and accuracy score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA\n",
    "Run the PCA algorithm on this dataset:\n",
    "1) Run the PCA algorithm with n = 2 principal components and determine what percent of the variance of the original dataset is returned by the transformed dataset.\n",
    "2) Try out a different number of n_components: we are seeking a balance of simplicity and maximal retained variance (Hint: Run a loop over different n_components values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run PCA with n_components = 2 on the dataframe of scaled features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measure the amount of explained variance of this result using the .explained_variance_ attribute\n",
    "#Explained variance of the model is the sum of explained variance of each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PCA to transform the dataframe into a new dataframe that is two principal components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use matplotlib to create a scatter plot of the data on these two principal components\n",
    "\n",
    "#If we were to use only 2 principal components we could easily visualize the data\n",
    "\n",
    "#col = np.where(df_components['target']==1,'k',np.where(df_components['target']==0,'b','r'))\n",
    "#matplotlib.pyplot.scatter(df_components['PC1'],df_components['PC2'],c=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a loop for number of components to determine which number maximizes the dataset's explained variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-run PCA on the optimal number of components. Transform the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-run Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the classification model using the reduced-dimension dataset as the training data. Do you observe any difference in the training time or accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-run basic Logistic regression model:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
